# ğŸ§  Web Dataset Processing Pipelines (Inspired by Datatrove)

This project provides a set of data processing pipelines for large-scale web datasets.
It is **heavily inspired by the `examples/` directory in [Hugging Face Datatrove](https://github.com/huggingface/datatrove)**, with customization for educational purposes.

Our pipelines are implemented using the `datatrove` framework and cover two major workflows:

- URL-based deduplication using signature comparison
- FineWeb-style dataset processing with quality filtering and minhash deduplication

---

## ğŸ“¦ Project Structure

â”œâ”€â”€ examples              #Full pipeline for processing data (FineWeb style)
â”œâ”€â”€ README.md                          # Project overview and usage guide
â””â”€â”€ LICENSE

## ğŸ” 1. URL Deduplication Pipeline

A three-step local pipeline that:

Reads input JSONL files and generates normalized URL signatures

Detects duplicate URLs using signature matching

Filters out duplicate documents

ğŸš€ Run locally:

```bash
python url_dedup_pipeline.py <input_folder> <base_output_folder> <sigs_dup_folder>
```

Output includes:

Deduplicated data in base_output_folder/output/

Removed duplicates in base_output_folder/removed/

## ğŸŒ 2. FineWeb-Style SLURM Pipeline

A scalable processing pipeline for Common Crawl data, similar to FineWeb.
It performs:

### ğŸ”¹ Base Processing:

- WARC reading from S3
- URL filtering
- HTML content extraction via Trafilatura
- Language and quality filtering
- Output to cleaned JSONL

### ğŸ”¹ Minhash Deduplication:

- Minhash signature generation
- Bucketing and clustering
- Final deduplication and token counting

### ğŸš€ Run on SLURM:

```python

# Inside fineweb_processing_pipeline.py
main_processing_executor.run()
stage4.run()
```

Other intermediate stages (signature, bucket, cluster) run automatically via `depends=` logic.

Make sure to update the following variables before running:

- DUMP_TO_PROCESS
- MAIN_OUTPUT_PATH

## ğŸ› ï¸ Requirements

Install the official datatrove library:

```bash
pip install git+https://github.com/huggingface/datatrove.git
```

Other dependencies:

- Python 3.8+
- NumPy
- SLURM (for distributed processing)

## ğŸ“š Inspiration & Attribution

This repository is adapted from and inspired by the examples/ directory in:

## ğŸ‘‰ Hugging Face Datatrove GitHub Repository

We gratefully acknowledge their open-source contributions.

### ğŸ‘¨â€ğŸ’» Contributors

Guangxin, Ashley, Erica, Haoqing

ğŸ“œ License
This repository is for educational and non-commercial use only.
Please refer to the Datatrove license for upstream licensing terms.
