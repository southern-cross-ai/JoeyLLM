# JoeyLLM GPT-2 model Training Guide

This document explains how to train the custom JoeyLLM GPT-2 model.

## Prerequisites
- Python 3.10+
- NVIDIA GPU with CUDA 12.4 +
Pytorch with gpu support
- Required packages:
  ```bash
  pip install -r requirements.txt

## ğŸ“ Project Structure Overview
JoeyLLM/
â”œâ”€â”€ main.py                     # Entry point script: loads config and starts training
â”œâ”€â”€ requirements.txt            # Project dependencies
â””â”€â”€ src/
    â”œâ”€â”€ configs/
    â”‚   â””â”€â”€ config.yaml         # YAML configuration (Hydra) for model, training, logging

    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ dataset.py          # Loads and batches tokenized training data
    â”‚   â”œâ”€â”€ test_data.py        # Loads/handles validation or test datasets
    â”‚   â””â”€â”€ chunk.py            # Preprocessing script to split long sequences into chunks

    â”œâ”€â”€ model/
    â”‚   â”œâ”€â”€ joeyllm.py          # Custom GPT-2 model (transformers, decoder blocks, attention)
    â”‚   â””â”€â”€ test_model.py       # Unit tests or evaluation scripts for model components

    â”œâ”€â”€ tokenizer/
    â”‚   â”œâ”€â”€ train_tokenizer.py  # Trains a tokenizer using a raw text corpus
    â”‚   â””â”€â”€ test_tokenizer.py   # Validates tokenizer output and decoding accuracy

    â””â”€â”€ train/
        â”œâ”€â”€ loop.py             # Core training loop (epochs, logging, checkpointing)
        â””â”€â”€ optimizer.py        # Optimizer setup (AdamW)


## Monitor with Weights & Biases

-Before running training, login to Weights & Biases: `wandb login`

## Training on Single GPU
    python src/main.py
  

This will:

Load the custom GPT-2 model with chosen configuration

Load 25% of the Project_Gutenberg_Australia dataset

Begin training with full logging to Weights & Biases

Save checkpoints after every epoch
