# JoeyLLM GPT-2 model Training Guide

This document explains how to train the custom JoeyLLM GPT-2 model.

## Prerequisites
- Python 3.10+
- NVIDIA GPU with CUDA 12.4 +
Pytorch with gpu support
- Required packages:
  ```bash
  pip install -r requirements.txt

## ğŸ“ Project Structure Overview
JoeyLLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ config.yaml           # Hydra-compatible YAML config for model, training, logging  
â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py            # Loads and batches tokenized dataset for training  
â”‚   â”‚   â”œâ”€â”€ test_data.py          # Handles test/validation datasets  
â”‚   â”‚   â””â”€â”€ chunk.py              # Preprocesses raw token sequences into fixed-length chunks  
â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ joeyllm.py            # Core GPT-2 model architecture (decoder blocks, attention, etc.)  
â”‚   â”‚   â””â”€â”€ test_model.py         # Unit tests or experimental evaluation of the model  
â”‚
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â”œâ”€â”€ train_tokenizer.py    # Trains a custom tokenizer from raw text data  
â”‚   â”‚   â””â”€â”€ test_tokenizer.py     # Validates tokenizer behavior (encoding/decoding tests)  
â”‚
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ loop.py               # Training loop logic (epochs, logging, loss, etc.)  
â”‚   â”‚   â””â”€â”€ optimizer.py          # Optimizer and scheduler setup (e.g. AdamW)  
â”‚
â”‚   â””â”€â”€ main.py                   # Entry point script: sets up config, model, data, and runs training  
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies for the full pipeline  



## Monitor with Weights & Biases

-Before running training, login to Weights & Biases: `wandb login`

## Training on Single GPU
    python src/main.py
  

This will:

Load the custom GPT-2 model with chosen configuration

Load 25% of the Project_Gutenberg_Australia dataset

Begin training with full logging to Weights & Biases

Save checkpoints after every epoch
