{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8e669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SouthernCrossAI/JoeyLLM_Tokenizer\", use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4e0b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33befb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\n",
    "    \"HF_HOME\",\n",
    "    \"HF_DATASETS_CACHE\",\n",
    "    \"TRANSFORMERS_CACHE\",\n",
    "    \"HF_HUB_CACHE\",\n",
    "    \"HF_DATASETS_HOME\",  # deprecated\n",
    "]:\n",
    "    print(f\"{var} =\", os.getenv(var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Config\n",
    "CHUNK_SIZE = 512\n",
    "BUFFER_TEXT_SIZE = 1000  # Number of samples to buffer before tokenizing (tune this)\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73433430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŒŠ Load streaming dataset\n",
    "hf_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    data_dir=\"sample/10BT\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db400f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(hf_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferedStreamTokenChunkDataset(IterableDataset):\n",
    "    def __init__(self, hf_streaming_dataset, tokenizer, chunk_size, buffer_text_size=10000):\n",
    "        self.dataset = hf_streaming_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.buffer_text_size = buffer_text_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        token_buffer = []\n",
    "\n",
    "        for example in self.dataset:\n",
    "            buffer.append(example[\"text\"])\n",
    "            if len(buffer) >= self.buffer_text_size:\n",
    "                tokenized = self.tokenizer(\n",
    "                    \" \".join(buffer),\n",
    "                    return_attention_mask=False,\n",
    "                    return_token_type_ids=False,\n",
    "                    add_special_tokens=False,\n",
    "                )[\"input_ids\"]\n",
    "                token_buffer.extend(tokenized)\n",
    "                buffer = []\n",
    "\n",
    "                while len(token_buffer) >= self.chunk_size + 1:\n",
    "                    input_ids = token_buffer[:self.chunk_size]\n",
    "                    target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "                    }\n",
    "\n",
    "                    token_buffer = token_buffer[self.chunk_size:]\n",
    "\n",
    "        # Final flush\n",
    "        if buffer:\n",
    "            tokenized = self.tokenizer(\n",
    "                \" \".join(buffer),\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "                add_special_tokens=False,\n",
    "            )[\"input_ids\"]\n",
    "            token_buffer.extend(tokenized)\n",
    "\n",
    "        while len(token_buffer) >= self.chunk_size + 1:\n",
    "            input_ids = token_buffer[:self.chunk_size]\n",
    "            target_ids = token_buffer[1:self.chunk_size + 1]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "            token_buffer = token_buffer[self.chunk_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BufferedStreamTokenChunkDataset(\n",
    "    hf_streaming_dataset=hf_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    buffer_text_size=BUFFER_TEXT_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e985c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adcbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(one_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_batch['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0483156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(one_batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = one_batch['input_ids'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd261752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c36abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(one_batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ec2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
